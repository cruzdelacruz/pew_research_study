{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "**memory friendly** - streams data instead of loading to memory <br>\n",
    "**cpu utilzation** - uses multithreading/processing to parallelize functions\n",
    "\n",
    "for building queries or doing data transformations/joining, I imagine it keeps a record of the things relevant to a query (probably disctionaries) and performs transformations or calculations on the fly (additional processes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "**Dictionaries are easy to work with but not memory-efficient!** <br>\n",
    "\n",
    "dictionary values can be stored anywhere in memory and accessing elements by keys can be slow.\n",
    "\n",
    "**lists are memory efficient because you access elements by index instead of by keys!** and list elements are stored adjacent to each other in memory so going from one element to another is easier. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "I want to use Pew Research Survey Data to explore data engineering topics/methodologies.\n",
    "specifically I wanted to showcase some columnar-based approaches to loading and analyzing datasets, raw pythonic ways to merge and correlate data and then the Polars (or other library) equivalents.\n",
    "\n",
    "I think that by showing how the methodology can be reproduced without extra libraries, we can gain a deeper and more intutitive and ingrained understanding of why we use the tools we do, as opposed to just memorizing how to do things with said tools.\n",
    "\n",
    "**Citation: Pew Research Centerâ€™s American Trends Panel**  <br>\n",
    "**The opinions expressed herein, including any implications for policy, are those of the author and not of Pew Research Center.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "#### What is in a Pew Serearch Survery Dataset?\n",
    "\n",
    "1. Survey results in the form of a csv\n",
    "2. An excel workbook with they survey codes (values and what questions/answers they represent)\n",
    "3. PDF of the questionairre\n",
    "4. PDF of the survery methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "I can see that we'll need to do some correlation or merging/lookup-type operations between item 1 and 2 (fun stuff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "#### Let's open the excel sheet and see what is in it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from openpyxl import load_workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_workbook = Path(r'D:\\VisualStudio\\Python\\public_projects\\polars_research\\datasets\\W139_Dec23\\ATP W139 Codebook.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# establish good practices from the start!\n",
    "\n",
    "if not path_to_workbook.exists():\n",
    "    raise FileNotFoundError(f'{path_to_workbook} does not exist')\n",
    "\n",
    "try:\n",
    "    workbook = load_workbook(filename=path_to_workbook, data_only=True, read_only=True)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "workbook.sheetnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_sheet = workbook['Codebook']\n",
    "\n",
    "for row in active_sheet.iter_rows():\n",
    "    for cell in row:\n",
    "        print(cell.coordinate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "Something is off about this workbook (**Iterating through all rows but it only returns data from the first cell**).. <br>so to debug let's try to open it with pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    codebook_df = pd.read_excel(path_to_workbook, sheet_name=None)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "finally:\n",
    "    print(codebook_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "Interesting, that worked... we could have a separate project to go over handling xlsx files with python.. for now, just to get the ball rolling we'll use pandas to load this lookup table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#codebook_df is a dictionary of dataframes. each key is a sheet for each sheet,\n",
    "#there is a dataframe to represent the table at that sheet\n",
    "\n",
    "for sheet, sheet_df in codebook_df.items():\n",
    "    display(sheet_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "#### What is this lookup table??\n",
    "\n",
    "I think for this to make sense, we have to take a peek at the survey data csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "atp_w139_survey_data = Path(r'D:\\VisualStudio\\Python\\public_projects\\polars_research\\datasets\\W139_Dec23\\ATP W139.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "let's read the first couple of rows only... <br>\n",
    "keep in mind we can read the filesize, but for the following reason, let's assume we don't know <br>\n",
    "what the actual memory footprint would be of loading the entire dataset into memory (reading the entire file <br>\n",
    "or loading it all into a pandas dataframe) <br>\n",
    "\n",
    "1. when you load a table into a pandas dataframe, it all gets loaded into memory. <br> for small files this is okay, but for anything larger than a few GB, it could blow up your memory... <br> everything in a Pandas dataframe is a python object. each object contains some amount of metadata or builtin methods or other things that take up additional memory than just the raw data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not atp_w139_survey_data.exists():\n",
    "    raise FileNotFoundError(f'{datetime.now()} {atp_w139_survey_data} does not exist')\n",
    "\n",
    "with open(atp_w139_survey_data, 'r', newline='', encoding='utf-8', errors='replace') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "\n",
    "    first_row = next(reader)\n",
    "    print(f'Number of columns: {len(list(first_row.keys()))}')\n",
    "    print(\"=\"*50,end=\"+\\n\")\n",
    "    pprint(first_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "Basically, each column name maps to a question (row) in the Codebook/lookup table, and each value corresponds to an answer. <br> Each row is a single interview from a single person."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "If you look back at the Codebook dataframe, you'll see some NaN (Not a Number) values (nulls, None) that's because the table was formatted by a human... some cells were merged. so when pandas reads it, it (I guess) unmerges the cells and keeps the value only in the first... which I think is what happens when we unmerge a cell in excel as well..\n",
    "\n",
    "so we have to fix the lookup table a bit before we start referencing it/correlating it to the survery data..\n",
    "\n",
    "now.. let's go back to that Codebook dataframe and save it as it's own CSV. then read it back and fix the NaN cells.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "codebook_dict = codebook_df['Codebook'].to_dict(orient='records')\n",
    "#first 10 rows:\n",
    "for i in range(10):\n",
    "    pprint(codebook_dict[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "we want to replace all the nan values with the correct label that corresponds to variable\n",
    "\n",
    "I tried fixing it by keeping a dict or strings to map each Variable to it's corresponding Variable_Label and then replacing nan values with a value from that map... I later learned this is called a 'fill forward' and for an ordered table like ours (ordered because we just read the excel sheet as someone made it, not modifying anything) we don't need anything more complex than a variable to hold the last non-nan value of the Value_Label column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "codebook_rows = [codebook_df['Codebook'].columns.tolist()] + codebook_df['Codebook'].values.tolist()\n",
    "codebook_rows[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_known_label = None\n",
    "\n",
    "for row in codebook_dict:\n",
    "    if not pd.isna(row['Variable_Label']):\n",
    "        last_known_label = row.get('Variable_Label')\n",
    "    else:\n",
    "        row.update({'Variable_Label':last_known_label})\n",
    "\n",
    "codebook_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_forward(rows: list[dict],col_to_ffill: str, output_path: Path):\n",
    "    last_known_label = None\n",
    "\n",
    "    for row in rows:\n",
    "        col_to_ffill_value = row.get(col_to_ffill,'NA')\n",
    "        if col_to_ffill_value == 'NA':\n",
    "            raise KeyError(col_to_ffill)\n",
    "        if not pd.isna(col_to_ffill_value):\n",
    "            last_known_label = row.get(col_to_ffill)\n",
    "        else:\n",
    "            row.update({col_to_ffill:last_known_label})\n",
    "        \n",
    "    with open(output_path, 'w', newline='', encoding='utf-8') as f:\n",
    "        headers = []\n",
    "        for row in rows:\n",
    "            for k in row.keys():\n",
    "                if k not in headers:\n",
    "                    headers.append(k)\n",
    "    \n",
    "        writer = csv.DictWriter(f, fieldnames=headers)\n",
    "        writer.writeheader()\n",
    "    \n",
    "        writer.writerows(codebook_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded_v_labels = ['Unique ID','Interview start time','Interview end time','Wave 139 weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lookup(rows: list[dict], primary_header: str):\n",
    "    survey_questions = {}\n",
    "    \n",
    "    for row in rows:\n",
    "        variable_label = row.get(primary_header, None)\n",
    "        \n",
    "        if not variable_label:\n",
    "            raise KeyError(primary_header)\n",
    "        \n",
    "        if variable_label not in survey_questions and variable_label not in excluded_v_labels:\n",
    "            codes = {\n",
    "                \"variable\":row.get('Variable'),\n",
    "                \"responses\":{\n",
    "                    str(row.get('Values')):row.get('Value_Labels')\n",
    "                }\n",
    "            }\n",
    "            survey_questions[variable_label] = codes\n",
    "        elif variable_label in survey_questions and variable_label not in excluded_v_labels:\n",
    "            if (val := str(row.get('Values'))) not in survey_questions[variable_label]['responses']:\n",
    "                survey_questions[variable_label]['responses'][val] = row.get('Value_Labels')\n",
    "    return survey_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_lookup = build_lookup(codebook_dict,'Variable_Label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "for q in question_lookup.keys():\n",
    "    print(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_responses(question: str):\n",
    "    response_rows = []\n",
    "    with open(atp_w139_survey_data, 'r', newline='', encoding='utf-8', errors='replace') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "    \n",
    "        #question_keys = list(question_lookup.keys())\n",
    "    \n",
    "        for row in reader:\n",
    "            response = row.get(question_lookup[question].get('variable'))\n",
    "            res_row = {\"Question\":question,\"Response\":question_lookup[question].get('responses').get(response)}\n",
    "            response_rows.append(res_row)\n",
    "    return pd.DataFrame(response_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = get_responses('Congress // Do you have a favorable or unfavorable opinion of each of the following?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_response(df, title: str):\n",
    "    response_counts = df['Response'].value_counts().reset_index()\n",
    "    response_counts.columns = ['Response', 'Count']\n",
    "    \n",
    "    fig = px.pie(response_counts,\n",
    "    names='Response',      # what labels each slice\n",
    "    values='Count',        # size of each slice\n",
    "    title=title,\n",
    "    width=600,\n",
    "    height=600)\n",
    "    \n",
    "    #fig.update_traces(textposition='outside')\n",
    "    fig.update_layout(\n",
    "        margin=dict(t=100),\n",
    "        title_font_size=10,\n",
    "        height=600,\n",
    "        width=600\n",
    "    )\n",
    "    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_iter = iter(question_lookup.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_question = next(questions_iter)\n",
    "visualize_response(get_responses(collect_question), collect_question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Polars Research",
   "language": "python",
   "name": "polars_research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
